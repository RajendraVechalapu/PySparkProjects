{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f66f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env file (recommended)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"REMOVED_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d63569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concise_error_message(e):\n",
    "    \"\"\"\n",
    "    Extract the most relevant single-line error message for display and AI use.\n",
    "    \"\"\"\n",
    "    tb_str = traceback.format_exception_only(type(e), e)\n",
    "    return tb_str[-1].strip() if tb_str else str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "27e46823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"REMOVED_API_KEY\")  # Replace with your actual key\n",
    "\n",
    "\n",
    "def ask_openai_fix(error_message):\n",
    "    \"\"\"\n",
    "    Ask OpenAI (gpt-4o-mini-2024-07-18) to explain and fix a PySpark error.\n",
    "    Prints the suggestion directly. Uses openai>=1.0.0 syntax.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "I encountered this PySpark error in a notebook:\n",
    "\n",
    "Error:\n",
    "{error_message}\n",
    "\n",
    "Please explain what went wrong and how to fix it clearly and concisely. Use a point-wise format.\n",
    "\n",
    "Respond only with:\n",
    "- ðŸ› ï¸ Root cause (1 short sentence)\n",
    "- âœ… Fix (1â€“2 steps max, include code if needed)\n",
    "- ðŸ’¡ Tip (1 short sentence to avoid it in future)\n",
    "\n",
    "Keep your answer under 6 lines.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        print(\"ðŸ§  AI Suggestion:\\n\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"âš ï¸ OpenAI API call failed: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_init import start_spark\n",
    "\n",
    "spark = start_spark(\"Spark Deep Learning Example\")\n",
    "\n",
    "data = [(\"Apple\", 10), (\"Banana\", 5), (\"Orange\", 8)]\n",
    "df = spark.createDataFrame(data, [\"Fruit\", \"Quantity\"])\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172fbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"USA\", 34),\n",
    "    (\"Bob\", \"USA\", 45),\n",
    "    (\"Catherine\", \"UK\", 29),\n",
    "    (\"David\", \"India\", 34),\n",
    "    (\"Emily\", \"India\", 21),\n",
    "    (\"Frank\", \"UK\", 45)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Country\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# âœ… Show the data\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "# âœ… Show the data\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, concat_ws\n",
    "\n",
    "df.select(\n",
    "    col(\"Name\"),\n",
    "    upper(col(\"Country\")).alias(\"Country_UPPER\"),\n",
    "    concat_ws(\" - \", col(\"Name\"), col(\"Country\")).alias(\"Combined\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be032cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"Age\") >= 45).show()\n",
    "df.filter((col(\"Age\") >= 45) & (col(\"Country\") == \"USA\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregation\n",
    "\n",
    "df.groupBy(\"Country\").count().show()\n",
    "df.groupBy(\"Country\").avg(\"Age\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02062beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 2 oldest people in each country\n",
    "from pyspark.sql import Window\n",
    "windowSpec = Window.partitionBy(\"Country\").orderBy(col(\"Age\").desc())\n",
    "\n",
    "#display(type(windowSpec))\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "df.withColumn(\"Rank\", row_number().over(windowSpec)) \\\n",
    "  .filter(col(\"Rank\") <= 2) \\\n",
    "  .select(\"Name\", \"Country\", \"Age\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregation\n",
    "df.groupBy(\"Country\").agg(\n",
    "    {\"Age\": \"avg\", \"Name\": \"count\"}\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99256479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Country\").orderBy(col(\"Age\").desc())\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding, removing, and renaming columns\n",
    "\n",
    "df = df.withColumn(\"AgePlus5\", col(\"Age\") + 5)\n",
    "df.show()\n",
    "\n",
    "df = df.drop(\"AgePlus5\")\n",
    "df = df.withColumnRenamed(\"Country\", \"Nation\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ordering the data and showing the top 3 oldest people\n",
    "df.orderBy(col(\"Age\").desc()).show(3)  # Top 3 oldest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a35c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create initial DataFrame\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 40)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Add 'Salary' column based on 'Age'\n",
    "df_with_null = df.withColumn(\"Salary\", col(\"Age\") * 1000)\n",
    "df_with_null = df_with_null.withColumn(\"Salary\", col(\"Salary\").cast(\"double\"))\n",
    "\n",
    "# âœ… Create new row with matching data types\n",
    "new_row = [(\"George\", None, None)]  # Ensure correct types: str, None (for int), None (for float)\n",
    "\n",
    "# âœ… Use schema explicitly to cast the new row\n",
    "df_new_row = spark.createDataFrame(new_row, schema=df_with_null.schema)\n",
    "\n",
    "# âœ… Union the DataFrames\n",
    "df_null = df_with_null.union(df_new_row)\n",
    "\n",
    "df_null.show()\n",
    "\n",
    "# # Fill missing salary values with 0\n",
    "df_null.fillna({\"Salary\": 0}).show()\n",
    "\n",
    "# # Drop rows with any nulls\n",
    "df_null.dropna().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# deduping data\n",
    "\n",
    "#Explain below code\n",
    "# Create a DataFrame with duplicate rows\n",
    "df_dup = df.union(df)\n",
    "df_dup.show()\n",
    "\n",
    "#quit()\n",
    "\n",
    "# explain the deduplication process\n",
    "# Deduplication in Spark can be done using the `dropDuplicates()` method.\n",
    "# This method removes duplicate rows based on all columns by default.\n",
    "# If you want to deduplicate based on specific columns, you can pass those column names as arguments.\n",
    "# Example of deduplication\n",
    "\n",
    "df_dup.dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e496e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_country_count_query():\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame with people data, registers it as a temporary SQL view,\n",
    "    and runs an SQL query to count the number of people per country.\n",
    "\n",
    "    Purpose:\n",
    "    - To demonstrate using SQL on Spark DataFrames via temporary views.\n",
    "    - Helpful for learners who are comfortable with SQL syntax.\n",
    "    \"\"\"\n",
    "    # Create sample DataFrame\n",
    "    data = [\n",
    "        (\"Alice\", \"USA\", 34),\n",
    "        (\"Bob\", \"USA\", 45),\n",
    "        (\"Catherine\", \"UK\", 29),\n",
    "        (\"David\", \"India\", 34),\n",
    "        (\"Emily\", \"India\", 21),\n",
    "        (\"Frank\", \"UK\", 45)\n",
    "    ]\n",
    "    columns = [\"Name\", \"Country\", \"Age\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Create a temporary SQL view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # Run SQL query to count by country\n",
    "    spark.sql(\"SELECT Country, COUNT(*) as total FROM people GROUP BY Country\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d0bfa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Country|total|\n",
      "+-------+-----+\n",
      "|    USA|    2|\n",
      "|     UK|    2|\n",
      "|  India|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    run_country_count_query()\n",
    "except Exception as e:\n",
    "    concise = get_concise_error_message(e)\n",
    "    #print(f\"\\nâŒ Error: {concise}\\n\")\n",
    "    print(\"ðŸ¤– Asking AI to help fix it...\\n\")\n",
    "    ask_openai_fix(concise)  # ðŸ‘ˆ This will print directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
