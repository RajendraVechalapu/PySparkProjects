{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b5d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_fix_helper import ask_openai_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d63569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "def get_concise_error_message(e):\n",
    "    \"\"\"\n",
    "    Extract the most relevant single-line error message for display and AI use.\n",
    "    \"\"\"\n",
    "    tb_str = traceback.format_exception_only(type(e), e)\n",
    "    return tb_str[-1].strip() if tb_str else str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41c2790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in d:\\rajendra-tech\\pysparkprojects\\.venv\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045f01c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark session started: Spark Deep Learning Example\n",
      "+------+--------+\n",
      "| Fruit|Quantity|\n",
      "+------+--------+\n",
      "| Apple|      10|\n",
      "|Banana|       5|\n",
      "|Orange|       8|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spark_init import start_spark\n",
    "\n",
    "spark = start_spark(\"Spark Deep Learning Example\")\n",
    "\n",
    "data = [(\"Apple\", 10), (\"Banana\", 5), (\"Orange\", 8)]\n",
    "df = spark.createDataFrame(data, [\"Fruit\", \"Quantity\"])\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172fbd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------+-------+---+\n",
      "|     Name|Country|Age|\n",
      "+---------+-------+---+\n",
      "|    Alice|    USA| 34|\n",
      "|      Bob|    USA| 45|\n",
      "|Catherine|     UK| 29|\n",
      "|    David|  India| 34|\n",
      "|    Emily|  India| 21|\n",
      "|    Frank|     UK| 45|\n",
      "+---------+-------+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âœ… Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"USA\", 34),\n",
    "    (\"Bob\", \"USA\", 45),\n",
    "    (\"Catherine\", \"UK\", 29),\n",
    "    (\"David\", \"India\", 34),\n",
    "    (\"Emily\", \"India\", 21),\n",
    "    (\"Frank\", \"UK\", 45)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Country\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# âœ… Show the data\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "# âœ… Show the data\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceeb8327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------+\n",
      "|     Name|Country_UPPER|      Combined|\n",
      "+---------+-------------+--------------+\n",
      "|    Alice|          USA|   Alice - USA|\n",
      "|      Bob|          USA|     Bob - USA|\n",
      "|Catherine|           UK|Catherine - UK|\n",
      "|    David|        INDIA| David - India|\n",
      "|    Emily|        INDIA| Emily - India|\n",
      "|    Frank|           UK|    Frank - UK|\n",
      "+---------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, concat_ws\n",
    "\n",
    "df.select(\n",
    "    col(\"Name\"),\n",
    "    upper(col(\"Country\")).alias(\"Country_UPPER\"),\n",
    "    concat_ws(\" - \", col(\"Name\"), col(\"Country\")).alias(\"Combined\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be032cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---+\n",
      "|     Name|Country|Age|\n",
      "+---------+-------+---+\n",
      "|    Alice|    USA| 34|\n",
      "|      Bob|    USA| 45|\n",
      "|Catherine|     UK| 29|\n",
      "|    David|  India| 34|\n",
      "|    Emily|  India| 21|\n",
      "|    Frank|     UK| 45|\n",
      "+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf5b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---+\n",
      "| Name|Country|Age|\n",
      "+-----+-------+---+\n",
      "|  Bob|    USA| 45|\n",
      "|Frank|     UK| 45|\n",
      "+-----+-------+---+\n",
      "\n",
      "+----+-------+---+\n",
      "|Name|Country|Age|\n",
      "+----+-------+---+\n",
      "| Bob|    USA| 45|\n",
      "+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Age\") >= 45).show()\n",
    "df.filter((col(\"Age\") >= 45) & (col(\"Country\") == \"USA\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a367dd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Country|count|\n",
      "+-------+-----+\n",
      "|    USA|    2|\n",
      "|     UK|    2|\n",
      "|  India|    2|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+--------+\n",
      "|Country|avg(Age)|\n",
      "+-------+--------+\n",
      "|    USA|    39.5|\n",
      "|     UK|    37.0|\n",
      "|  India|    27.5|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping and Aggregation\n",
    "\n",
    "df.groupBy(\"Country\").count().show()\n",
    "df.groupBy(\"Country\").avg(\"Age\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02062beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---+\n",
      "|     Name|Country|Age|\n",
      "+---------+-------+---+\n",
      "|    David|  India| 34|\n",
      "|    Emily|  India| 21|\n",
      "|    Frank|     UK| 45|\n",
      "|Catherine|     UK| 29|\n",
      "|      Bob|    USA| 45|\n",
      "|    Alice|    USA| 34|\n",
      "+---------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the top 2 oldest people in each country\n",
    "from pyspark.sql import Window\n",
    "windowSpec = Window.partitionBy(\"Country\").orderBy(col(\"Age\").desc())\n",
    "\n",
    "#display(type(windowSpec))\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "df.withColumn(\"Rank\", row_number().over(windowSpec)) \\\n",
    "  .filter(col(\"Rank\") <= 2) \\\n",
    "  .select(\"Name\", \"Country\", \"Age\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e846d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+\n",
      "|Country|avg(Age)|count(Name)|\n",
      "+-------+--------+-----------+\n",
      "|    USA|    39.5|          2|\n",
      "|     UK|    37.0|          2|\n",
      "|  India|    27.5|          2|\n",
      "+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping and Aggregation\n",
    "df.groupBy(\"Country\").agg(\n",
    "    {\"Age\": \"avg\", \"Name\": \"count\"}\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99256479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---+----+----------+\n",
      "|     Name|Country|Age|rank|dense_rank|\n",
      "+---------+-------+---+----+----------+\n",
      "|    David|  India| 34|   1|         1|\n",
      "|    Emily|  India| 21|   2|         2|\n",
      "|    Frank|     UK| 45|   1|         1|\n",
      "|Catherine|     UK| 29|   2|         2|\n",
      "|      Bob|    USA| 45|   1|         1|\n",
      "|    Alice|    USA| 34|   2|         2|\n",
      "+---------+-------+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Country\").orderBy(col(\"Age\").desc())\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b52b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---+--------+\n",
      "|     Name|Country|Age|AgePlus5|\n",
      "+---------+-------+---+--------+\n",
      "|    Alice|    USA| 34|      39|\n",
      "|      Bob|    USA| 45|      50|\n",
      "|Catherine|     UK| 29|      34|\n",
      "|    David|  India| 34|      39|\n",
      "|    Emily|  India| 21|      26|\n",
      "|    Frank|     UK| 45|      50|\n",
      "+---------+-------+---+--------+\n",
      "\n",
      "+---------+------+---+\n",
      "|     Name|Nation|Age|\n",
      "+---------+------+---+\n",
      "|    Alice|   USA| 34|\n",
      "|      Bob|   USA| 45|\n",
      "|Catherine|    UK| 29|\n",
      "|    David| India| 34|\n",
      "|    Emily| India| 21|\n",
      "|    Frank|    UK| 45|\n",
      "+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding, removing, and renaming columns\n",
    "\n",
    "df = df.withColumn(\"AgePlus5\", col(\"Age\") + 5)\n",
    "df.show()\n",
    "\n",
    "df = df.drop(\"AgePlus5\")\n",
    "df = df.withColumnRenamed(\"Country\", \"Nation\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1700cb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+\n",
      "| Name|Nation|Age|\n",
      "+-----+------+---+\n",
      "|Frank|    UK| 45|\n",
      "|  Bob|   USA| 45|\n",
      "|David| India| 34|\n",
      "+-----+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ordering the data and showing the top 3 oldest people\n",
    "df.orderBy(col(\"Age\").desc()).show(3)  # Top 3 oldest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a35c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|   Name| Age| Salary|\n",
      "+-------+----+-------+\n",
      "|  Alice|  30|30000.0|\n",
      "|    Bob|  25|25000.0|\n",
      "|Charlie|  40|40000.0|\n",
      "| George|NULL|   NULL|\n",
      "+-------+----+-------+\n",
      "\n",
      "+-------+----+-------+\n",
      "|   Name| Age| Salary|\n",
      "+-------+----+-------+\n",
      "|  Alice|  30|30000.0|\n",
      "|    Bob|  25|25000.0|\n",
      "|Charlie|  40|40000.0|\n",
      "| George|NULL|    0.0|\n",
      "+-------+----+-------+\n",
      "\n",
      "+-------+---+-------+\n",
      "|   Name|Age| Salary|\n",
      "+-------+---+-------+\n",
      "|  Alice| 30|30000.0|\n",
      "|    Bob| 25|25000.0|\n",
      "|Charlie| 40|40000.0|\n",
      "+-------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create initial DataFrame\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Charlie\", 40)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Add 'Salary' column based on 'Age'\n",
    "df_with_null = df.withColumn(\"Salary\", col(\"Age\") * 1000)\n",
    "df_with_null = df_with_null.withColumn(\"Salary\", col(\"Salary\").cast(\"double\"))\n",
    "\n",
    "# âœ… Create new row with matching data types\n",
    "new_row = [(\"George\", None, None)]  # Ensure correct types: str, None (for int), None (for float)\n",
    "\n",
    "# âœ… Use schema explicitly to cast the new row\n",
    "df_new_row = spark.createDataFrame(new_row, schema=df_with_null.schema)\n",
    "\n",
    "# âœ… Union the DataFrames\n",
    "df_null = df_with_null.union(df_new_row)\n",
    "\n",
    "df_null.show()\n",
    "\n",
    "# # Fill missing salary values with 0\n",
    "df_null.fillna({\"Salary\": 0}).show()\n",
    "\n",
    "# # Drop rows with any nulls\n",
    "df_null.dropna().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353e8d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 30|\n",
      "|    Bob| 25|\n",
      "|Charlie| 40|\n",
      "|  Alice| 30|\n",
      "|    Bob| 25|\n",
      "|Charlie| 40|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 30|\n",
      "|    Bob| 25|\n",
      "|Charlie| 40|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# deduping data\n",
    "\n",
    "#Explain below code\n",
    "# Create a DataFrame with duplicate rows\n",
    "df_dup = df.union(df)\n",
    "df_dup.show()\n",
    "\n",
    "#quit()\n",
    "\n",
    "# explain the deduplication process\n",
    "# Deduplication in Spark can be done using the `dropDuplicates()` method.\n",
    "# This method removes duplicate rows based on all columns by default.\n",
    "# If you want to deduplicate based on specific columns, you can pass those column names as arguments.\n",
    "# Example of deduplication\n",
    "\n",
    "df_dup.dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_country_count_query():\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame with people data, registers it as a temporary SQL view,\n",
    "    and runs an SQL query to count the number of people per country.\n",
    "\n",
    "    Purpose:\n",
    "    - To demonstrate using SQL on Spark DataFrames via temporary views.\n",
    "    - Helpful for learners who are comfortable with SQL syntax.\n",
    "    \"\"\"\n",
    "    # Create sample DataFrame\n",
    "    data = [\n",
    "        (\"Alice\", \"USA\", 34),\n",
    "        (\"Bob\", \"USA\", 45),\n",
    "        (\"Catherine\", \"UK\", 29),\n",
    "        (\"David\", \"India\", 34),\n",
    "        (\"Emily\", \"India\", 21),\n",
    "        (\"Frank\", \"UK\", 45)\n",
    "    ]\n",
    "    columns = [\"Name\", \"Country\", \"Age\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Create a temporary SQL view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # Run SQL query to count by country\n",
    "    spark.sql(\"SELECT Country, COUNT(*) as total FROM people GROUP BY Country\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0bfa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Asking AI to help fix it...\n",
      "\n",
      "ðŸ§  AI Suggestion:\n",
      "\n",
      "- ðŸ› ï¸ Root cause: The SQL query is missing a comma between the `Country` column and the `COUNT(*)` function.  \n",
      "- âœ… Fix: Add a comma after `Country`. The corrected query is:  \n",
      "  ```sql\n",
      "  SELECT Country, COUNT(*) as total FROM people GROUP BY Country\n",
      "  ```  \n",
      "- ðŸ’¡ Tip: Always check for commas between selected columns in SQL queries to avoid syntax errors.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    run_country_count_query()\n",
    "except Exception as e:\n",
    "    concise = get_concise_error_message(e)\n",
    "    #print(f\"\\nâŒ Error: {concise}\\n\")\n",
    "    print(\"ðŸ¤– Asking AI to help fix it...\\n\")\n",
    "    ask_openai_fix(concise)  # ðŸ‘ˆ This will print directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a94631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
